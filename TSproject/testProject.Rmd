---
title: "Time Series Analysis and Forecasting for Technical Olympic S.A. Stock Closing Values"
output:
  html_document:
    df_print: paged
  pdf_document: default
  word_document: default
---

<p>This project involves a comprehensive time series analysis and forecasting endeavor focused on the closing values of Technical Olympic S.A. stock. The dataset, representing historical stock prices, is loaded and preprocessed using the R programming language. The analysis encompasses various stages, including time series decomposition to uncover trends, seasonality, and residuals. Additionally, a polynomial approach is applied to explore the fit of polynomial models of different degrees. The Box-Cox transformation is employed to stabilize variance, and the Box-Jenkins method, involving the Augmented Dickey-Fuller and KPSS tests, is used to address stationarity and identify optimal parameters for ARIMA modeling. The project concludes with the fitting of ARIMA models, diagnostic checks of residuals, and the generation of forecasts for future stock closing values. Through these analytical steps, the aim is to provide insights into the underlying patterns of Technical Olympic S.A. stock prices and to make informed predictions for future market behavior.<p>

```{r Working directory, message=FALSE, warning=FALSE, include=TRUE, paged.print=FALSE}
# Loading required packages
library(knitr)
library(stats)
library(MARSS)
library(forecast)
library(datasets)
library(tseries)
```

```{r echo=FALSE, fig.width=8, message=FALSE, warning=FALSE, paged.print=TRUE}
# Loading data
data=read.csv('data.csv')
data$date=as.Date(data$date,format="%d/%m/%y")
data=data.frame('date'=data$date,'value'=data$value)
r=range(data$value)[2]-range(data$value)[1]
```

```{r echo=FALSE, fig.width=8, message=FALSE, warning=FALSE, paged.print=TRUE}
#Time series
f=25
a=1
b=1
value_ts=ts(data$value,frequency = f,start = a)
```

```{r echo=FALSE, fig.width=10, message=FALSE, warning=FALSE, paged.print=TRUE}
#Plot
plot.ts(value_ts, main="Time series plot",ylab='value',col='#69b3a2',bty='n')
```

<h4>Decomposition of time series</h4>
$$x_t=m_t+s_t+e_t$$
<p>Trend: $\hat{m}_t=\sum_{k=-a}^{a}{\left(\dfrac{1}{1+2a}\right)x_{t+k}}$</p>

<p>Seasonality: $\hat{s}_t=x_t-\hat{m}_t$</p>

<p>Residuals: $\hat{e}_t=x_t-\hat{m}_t-\hat{s}_t$</p>


```{r echo=FALSE, fig.width=10, message=FALSE, warning=FALSE, paged.print=TRUE}
par(mfrow=c(1,2))
value_dec=decompose(value_ts)
plot(value_dec, yax.flip = TRUE,col='#69b3a2',bty='n')
```

<h4>Polynomial approach</h4>

```{r echo=FALSE, fig.width=10, message=FALSE, warning=FALSE, paged.print=FALSE}
n=4
par(mfrow=c(2,2))
t=1:length(value_ts)
for (i in 1:n){
  fit_poln=lm(value_ts ~ poly(t,i))
  print(paste('Degree of polynomial: ',i))
  print(paste('adj R squared: ',summary(fit_poln)$adj.r.squared))
  print(paste('AIC: ',AIC(fit_poln)))
  print(paste('BIC: ',BIC(fit_poln)))
  plot(value_ts,ylab='value',col='#69b3a2',bty='n')
  lines(ts(fit_poln$fitted.values,frequency = f,start = a),type="l",col="red",lwd = 1,pch = 10)
}
par(mfrow=c(1,1))
fit_poln=lm(value_ts ~ poly(t,3)) #Selection of the degree of the polynomial
value_pol=ts(value_ts-fit_poln$fitted.values,frequency = f,start = a)
plot.ts(value_pol,col='orange',main='Residuals',ylab='res',bty='n',ylim=c(-r/2,r/2))
```

<p>Based on the provided results, the polynomial of degree 3 seems to strike a good balance between model complexity and goodness of fit, as it has a relatively high adjusted R-squared and lower AIC/BIC compared to higher-degree polynomials.</p>

<h4>Box-Jenkins method</h4>

<h5>Stagnation check</h5>

```{r echo=FALSE, fig.width=10, message=FALSE, warning=FALSE, paged.print=TRUE}
adf.test(value_ts)  # p<0.05
```
<p>The ADF test suggests non-stationarity, but it doesn't provide strong evidence.</p>
```{r echo=FALSE, fig.width=10, message=FALSE, warning=FALSE, paged.print=TRUE}
kpss.test(value_ts)  # p>0.05
```
<p>The KPSS test suggests non-stationarity and provides stronger evidence against stationarity.</p>

<h5>Differences transformation</h5>

<p>We apply the difference transformation until the Augmented Dickey-Fuller Test (adf) gives \(p-value<0.05\)</p>

```{r echo=FALSE, fig.width=10, message=FALSE, warning=FALSE, paged.print=TRUE}
l=1 # Elimination of seasonality
d=ndiffs(value_ts, test = "adf") # Elimination of trend
value_dif=diff(value_ts,lag=l,differences = d)
plot.ts(value_dif,main='Residuals',ylab='res',col='orange',ylim=c(-r/2,r/2),bty='n')
```

<h5>Autocorrelation - heteroscedasticity</h5>

```{r echo=FALSE, fig.width=10, message=FALSE, warning=FALSE, paged.print=TRUE}
par(mfrow=c(1,2))
plot(acf(value_ts,plot = F),main = 'Auto-Correlation Function Estimation',col='orange',bty='n')
q=11
plot(pacf(value_ts,plot = F),main = 'Partial Auto-Correlation Function Estimation',col='orange',bty='n')
p=1
```

<h5>Auto-regressive model \(AR(p)\)</h5>

```{r echo=FALSE, fig.width=10, message=FALSE, warning=FALSE, paged.print=TRUE}
par(mfrow=c(1,2))
fit_ar=arima(value_ts,order = c(p,0,0),method = 'CSS')
value_ar=fitted(fit_ar)
plot.ts(value_ts,ylab='value',main='AR(p) model',col='#69b3a2',bty='n')
lines(value_ar,col='red')
legend('bottomright',legend=c('observed','fitted'), col=c('#69b3a2','red'),pch=c('-','-'),bty='n')
plot.ts(value_ts-value_ar, yax.flip = TRUE,col='orange',main='Residuals',ylim=c(-r/2,r/2),ylab='res',bty='n')
```

<h5>Moving Average model \(MA(q)\)</h5>

```{r echo=FALSE, fig.width=10, message=FALSE, warning=FALSE, paged.print=TRUE}
par(mfrow=c(1,2))
fit_ma=arima(value_ts,order = c(0,0,q),method = 'CSS')
value_ma=fitted(fit_ma)
plot.ts(value_ts,ylab='value',main='MA(q) model',col='#69b3a2',bty='n')
lines(value_ma,col='red')
legend('bottomright',legend=c('observed','fitted'), col=c('#69b3a2','red'),pch=c('-','-'),bty='n')
plot.ts(value_ts-value_ma, yax.flip = TRUE,col='orange',main='Residuals',ylab='res',ylim=c(-r/2,r/2),bty='n')
```

<h5>Auto-regressive Integrated Moving Average model \(ARIMA(p,d,q)\)</h5>

```{r echo=FALSE, fig.width=10, message=FALSE, warning=FALSE, paged.print=TRUE}
par(mfrow=c(1,2))
fit_arima=arima(value_ts,order = c(p,d,q),method = 'CSS')
value_arima=fitted(fit_arima)
plot.ts(value_ts,ylab='value',main='ARIMA(p,d,q) model',col='#69b3a2',bty='n')
lines(value_arima,col='red')
legend('bottomright',legend=c('observed','fitted'), col=c('#69b3a2','red'),pch=c('-','-'),bty='n')
plot.ts(value_ts-value_arima, yax.flip = TRUE,col='orange',main='Residuals',ylab='res',ylim=c(-r/2,r/2),bty='n')
```

<h5>Residuals check</h5>

```{r echo=FALSE, fig.width=10, message=FALSE, warning=FALSE, paged.print=TRUE}
checkresiduals(fit_arima)  #h0 independence (p>0.05)
```

<h5>Predictions</h5>

```{r echo=FALSE, fig.width=10, message=FALSE, warning=FALSE, paged.print=TRUE}
frcast=forecast(fit_arima, h = 7)
kable(frcast)
plot(frcast,main="Prediction",ylab='value',col='#69b3a2',bty='n')
kable(accuracy(frcast$fitted, data$value))
```
* The ME being close to zero suggests that, on average, the forecasts are relatively balanced in terms of overestimation and underestimation.
* The RMSE provides an overall measure of the accuracy, with a lower value indicating better performance. The value of 0.01353461 suggests a relatively small typical error in your forecasts.
* The MAE and MAPE provide additional measures of the average magnitude and percentage difference of errors, respectively.
* The MPE being positive suggests a tendency for overestimation on average.